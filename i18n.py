TRANSLATIONS = {
    'en': {
        'app_title': 'TinyNetLab',
        'select_app': 'Select App',
        'language': 'Language',
        'moons_explorer': 'Moons Explorer',
        'tinynet_trainer': 'TinyNet Trainer',
        'moons_title': 'Moons Visualization',
        'samples': 'Samples',
        'noise': 'Noise',
        'show_linear_model': 'Show linear model',
        'moons_dataset': 'Moons Dataset',
        'log_reg_loss': 'Logistic Regression loss: {loss:.4f}',
        'linear_model_note': 'A linear model can only learn a straight decision boundary and therefore cannot correctly separate the curved moons.',
        'moons_think': '**Think**\n\n- Why can\'t a single straight line separate the two "moons"?\n- If using only a linear model such as logistic regression, roughly how low will the loss get? Try verifying your guess.\n\nHint: The linear model can only draw a straight boundary, while the moons dataset is curved, so the loss can\'t get very low.',
        'moons_explore_page_title': 'Moons Data Explorer',
        'moons_distribution_title': 'Moons Data Distribution',
        'tiny_title': 'TinyNet {n_input}-{n_hidden}-{n_output}',
        'tinynet_description': 'Train a tiny neural network on the moons dataset and view the decision boundary.',
        'hidden_units': 'Hidden units',
        'epochs': 'Epochs',
        'learning_rate': 'Learning rate',
        'train': 'Train',
        'final_loss': 'Final loss: {loss:.4f}',
        'decision_boundary': 'Decision Boundary',
        'loss_over_time': 'Loss over Time',
        'attention_demo': 'Attention Demo',
        'select_sentence': 'Select sentence',
        'dimension': 'Dimension',
        'randomize_vectors': 'Randomize vectors',
        'show_attention_matrix': 'Show attention matrix',
        'attention_matrix': 'Attention Matrix',
        'vector_transform': 'Vector transform',
        'use_positional_encoding': 'Use positional encoding',
        'show_positional_encoding': 'Show positional encoding',
        'positional_encoding': 'Positional Encoding',
        'vector_chart_help_title': 'What does vector_chart show?',
        'vector_chart_help': 'The blue points are original vectors, the red points are vectors after attention, and gray lines connect them. This chart helps you see how attention moves each token\'s vector.',
        'positional_encoding_help_title': 'What is positional encoding?',
        'positional_encoding_help': 'Positional encoding injects each token\'s index into its vector. When enabled, a sinusoidal vector based on position is added. Rows correspond to token positions and columns correspond to encoding dimensions. With length=4 and dimension=2, row 1 column 0 \u2248 0.84 from the sine term and column 1 \u2248 0.54 from the cosine term.',
        'attention_matrix_help_title': 'What is the attention matrix?',
        'attention_matrix_help': 'The attention matrix displays weights from scaled dot-product attention. Each value tells how strongly a query attends to a key, and each row sums to one.',
    },
    'zh': {
        'app_title': 'TinyNetLab',
        'select_app': '选择应用',
        'language': '语言',
        'moons_explorer': 'Moons 数据探索',
        'tinynet_trainer': 'TinyNet 训练器',
        'moons_title': 'Moons 数据可视化',
        'samples': '样本数',
        'noise': '噪声',
        'show_linear_model': '显示线性模型效果',
        'moons_dataset': 'Moons 数据集',
        'log_reg_loss': 'Logistic Regression 损失: {loss:.4f}',
        'linear_model_note': '线性模型只能学习直线决策边界，因此无法正确分开弯曲的 moons 数据。',
        'moons_think': '**思考**\n\n- 画出来的两条“月牙”为什么一条直线分不开？\n- 如果只有线性模型（如 logistic regression），损失大概会降到哪？也可以试试验证你的猜想。\n\n提示：线性模型只能画出直线决策边界，而 moons 数据集呈现弯曲月牙形状，因此线性模型无法获得很低的损失。',
        'moons_explore_page_title': 'Moons 数据探索',
        'moons_distribution_title': 'Moons 数据分布',
        'tiny_title': 'TinyNet {n_input}-{n_hidden}-{n_output}',
        'tinynet_description': '训练一个小型神经网络并查看决策边界。',
        'hidden_units': '隐藏单元数',
        'epochs': '训练轮数',
        'learning_rate': '学习率',
        'train': '开始训练',
        'final_loss': '最终损失: {loss:.4f}',
        'decision_boundary': '决策边界',
        'loss_over_time': '损失曲线',
        'attention_demo': 'Attention 演示',
        'select_sentence': '选择句子',
        'dimension': '维度',
        'randomize_vectors': '随机初始化向量',
        'show_attention_matrix': '显示注意力矩阵',
        'attention_matrix': '注意力矩阵',
        'vector_transform': '向量变化',
        'use_positional_encoding': '使用位置编码',
        'show_positional_encoding': '显示位置编码',
        'positional_encoding': '位置编码',
        'vector_chart_help_title': 'vector_chart 展示什么？',
        'vector_chart_help': '在 attention_demo.py 中，vector_chart 函数用于可视化注意力机制前后的 2D 向量变化。蓝点表示原始向量，红点表示注意力后的向量，灰线连接同一 token 的起点和终点，可帮助你观察注意力如何移动每个向量。',
        'positional_encoding_help_title': '什么是位置编码？',
        'positional_encoding_help': '位置编码用于在向量中加入每个 token 的位置信息。勾选“使用位置编码”后，本示例会按索引加上正弦波向量。行对应 token 位置，列对应编码维度。默认 length=4、dimension=2 时，第 1 行第 0 列约为 0.84（正弦项），第 1 列约为 0.54（余弦项）。',
        'attention_matrix_help_title': '什么是注意力矩阵？',
        'attention_matrix_help': '注意力矩阵展示了缩放点积注意力得到的权重。每个数值表示 query token 对 key token 的关注程度，每行权重之和为 1。',
    }
}

import streamlit as st


def t(key: str, **kwargs) -> str:
    """Return the translated string based on current language."""
    lang = st.session_state.get("lang", "en")
    template = TRANSLATIONS.get(lang, TRANSLATIONS['en']).get(key, key)
    return template.format(**kwargs)
